0. Data
1. Vocab (BPE)
1 days
2. Models -> Encoder, Decoder, Disc
3 days
3. GAN training
4. Autoencoder loss
5. Main training loop
1 week
6. Generate (Beam search)
7. Training!
3 days

x = BPE(sent)
x = embedding (x)
Encoder (x) -> h = h_1, ..., h_n
    *Decoder(h) -> y
    Disc(h) -> style

y_1, y_2, (x_1, x_2) -> 

[h_1, ..., h_n] + (y_1, y_2, ?) #
[h_1, ..., h_n] + (x_1, x_2, ?) #


Dataset + Dataloader: should
0. Use datasets to load train/dev/test data
1. sample a number of sentences from trainset along with their style labels
2. tokenize and index them (with BPE and Vocab class)
3. pad them (data collator)
4. store original lengths of [tokenized] sentences in seq_lens
5. return (batch[BxT], seq_lens[B])

In the decoder, we should also pass a causal mask (don't see future) for the target sequence, and a padding mask for encoder and decoder

Create a supervised test dataset

[x_1, ... x_n] -> enc -> [H, h_1, .. , h_n] -> decoder -> [y_1, ..., y_n]
[x_1, ... x_n] -> enc -> [h_1, .. , h_n] -> disc -> class label

TODO:
X saving and loading model every epoch 
X. greedy search for testing
X. beam search for testing
X. tune discriminator updates vs. encoder updates[only update encoder if discriminator's loss is low] (first, train Autoencoder for a couple epochs, then upgrade disc with every batch and encoder once every three batches)
X. tune lambda for adv vs reconstruction loss (lambda = 10 so that adv loss is more bold)
5. add noise to disc input
6. compute validation (reconstruction and disc) loss, disc acc and report every epoch
X. logging with tensorboard (log every 100 batches or so)
8. compute bleu score for testing
X. After every epoch, generate sentences for opposite and same style to see how the model is performing
X. delete unused tensors
X. Dataloader num_workers
12. 50 Test set sentences -> change sentiment -> BLEU

Logging:
reconstruction loss, disc acc, disc loss