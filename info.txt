0. Data
1. Vocab (BPE)
1 days
2. Models -> Encoder, Decoder, Disc
3 days
3. GAN training
4. Autoencoder loss
5. Main training loop
1 week
6. Generate (Beam search)
7. Training!
3 days

x = BPE(sent)
x = embedding (x)
Encoder (x) -> h = h_1, ..., h_n
    *Decoder(h) -> y
    Disc(h) -> style

y_1, y_2, (x_1, x_2) -> 

[h_1, ..., h_n] + (y_1, y_2, ?) #
[h_1, ..., h_n] + (x_1, x_2, ?) #


Dataset + Dataloader: should
0. Use datasets to load train/dev/test data
1. sample a number of sentences from trainset along with their style labels
2. tokenize and index them (with BPE and Vocab class)
3. pad them (data collator)
4. store original lengths of [tokenized] sentences in seq_lens
5. return (batch[BxT], seq_lens[B])

In the decoder, we should also pass a causal mask (don't see future) for the target sequence, and a padding mask for encoder and decoder

Create a supervised test dataset